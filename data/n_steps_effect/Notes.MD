Same training with same n_envs with different n_steps.

when env_steps 1024, it converged much faster and got better overall reward with same training.

