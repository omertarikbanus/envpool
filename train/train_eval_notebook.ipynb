{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a732c001",
   "metadata": {},
   "source": [
    "# Quadruped PPO Training and Evaluation with EnvPool\n",
    "\n",
    "This notebook provides a comprehensive pipeline for training and evaluating PPO (Proximal Policy Optimization) models for quadruped control using EnvPool. The code has been refactored to use common modules for better organization and reusability.\n",
    "\n",
    "## Features:\n",
    "- **Modular Design**: Common components moved to separate modules\n",
    "- **Interactive Training**: Train models with progress monitoring\n",
    "- **Comprehensive Evaluation**: Detailed performance metrics and visualizations\n",
    "- **Model Management**: Save/load models with normalization statistics\n",
    "- **Environment Support**: Multiple MuJoCo environments via EnvPool\n",
    "\n",
    "## Notebook Structure:\n",
    "1. Import libraries and setup\n",
    "2. Configure training parameters\n",
    "3. Create and setup environments\n",
    "4. Define model architecture\n",
    "5. Train the PPO model\n",
    "6. Save model and statistics\n",
    "7. Load and evaluate models\n",
    "8. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321ec52",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries including the newly refactored common modules for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from packaging import version\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning and RL libraries\n",
    "import torch as th\n",
    "import envpool\n",
    "import gym\n",
    "import gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Import our refactored common modules\n",
    "sys.path.append('./common')\n",
    "from common import (\n",
    "    VecAdapter,\n",
    "    setup_environment,\n",
    "    create_policy_kwargs,\n",
    "    create_ppo_model,\n",
    "    setup_logging,\n",
    "    create_or_load_model,\n",
    "    save_model_and_stats,\n",
    "    setup_vecnormalize,\n",
    "    find_vecnormalize_wrapper,\n",
    "    load_model_and_normalization,\n",
    "    detailed_evaluation,\n",
    "    print_evaluation_results,\n",
    "    save_evaluation_results\n",
    ")\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Force PyTorch to use one thread for speed\n",
    "th.set_num_threads(1)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üöÄ PyTorch version: {th.__version__}\")\n",
    "print(f\"üéØ EnvPool available: {envpool.__version__}\")\n",
    "print(f\"ü§ñ GPU available: {th.cuda.is_available()}\")\n",
    "if th.cuda.is_available():\n",
    "    print(f\"   GPU: {th.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {th.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8345a9",
   "metadata": {},
   "source": [
    "## 2. Setup Environment Configuration\n",
    "\n",
    "Configure training parameters, environment settings, and logging for reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee05689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Environment settings\n",
    "ENV_NAME = \"Humanoid-v4\"  # Try: \"Ant-v4\", \"HalfCheetah-v4\", \"Hopper-v4\", \"Walker2d-v4\"\n",
    "NUM_ENVS = 256            # Number of parallel environments (higher = faster training)\n",
    "SEED = 42                 # Random seed for reproducibility\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_TIMESTEPS = 1_000_000  # Total training steps (adjust for shorter/longer training)\n",
    "USE_VECNORMALIZE = True      # Use observation/reward normalization (recommended)\n",
    "RENDER_MODE = False          # Set to True for visualization during training (slower)\n",
    "\n",
    "# Model save/load settings\n",
    "MODEL_SAVE_PATH = \"./models/quadruped_ppo_model\"\n",
    "FORCE_NEW = False           # Set True to always start fresh training\n",
    "CONTINUE_TRAINING = False   # Set True to automatically continue from existing model\n",
    "\n",
    "# Evaluation settings\n",
    "N_EVAL_EPISODES = 20       # Number of episodes for evaluation\n",
    "DETERMINISTIC_EVAL = True  # Use deterministic actions during evaluation\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP LOGGING AND REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "th.manual_seed(SEED)\n",
    "if th.cuda.is_available():\n",
    "    th.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging()\n",
    "logging.info(\"üöÄ Starting PPO training experiment\")\n",
    "logging.info(f\"Environment: {ENV_NAME}, Envs: {NUM_ENVS}, Seed: {SEED}\")\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"   Environment: {ENV_NAME}\")\n",
    "print(f\"   Parallel Envs: {NUM_ENVS}\")\n",
    "print(f\"   Training Steps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"   VecNormalize: {USE_VECNORMALIZE}\")\n",
    "print(f\"   Model Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"   Seed: {SEED}\")\n",
    "print(\"‚úÖ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a6aad",
   "metadata": {},
   "source": [
    "## 3. Create and Configure Training Environment\n",
    "\n",
    "Set up the EnvPool environment with proper wrappers and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE TRAINING ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üåç Creating training environment...\")\n",
    "\n",
    "# Create base environment using our utility function\n",
    "env = setup_environment(\n",
    "    env_name=ENV_NAME,\n",
    "    num_envs=NUM_ENVS,\n",
    "    seed=SEED,\n",
    "    render_mode=RENDER_MODE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Base environment created: {ENV_NAME}\")\n",
    "print(f\"   Action space: {env.action_space}\")\n",
    "print(f\"   Observation space: {env.observation_space}\")\n",
    "print(f\"   Number of environments: {env.num_envs}\")\n",
    "\n",
    "# Apply VecNormalize wrapper if requested\n",
    "env, vecnormalize_wrapper = setup_vecnormalize(env, USE_VECNORMALIZE)\n",
    "if USE_VECNORMALIZE:\n",
    "    print(\"‚úÖ VecNormalize wrapper applied\")\n",
    "\n",
    "# Apply monitoring wrapper\n",
    "env = VecMonitor(env)\n",
    "print(\"‚úÖ VecMonitor wrapper applied\")\n",
    "\n",
    "# Display environment information\n",
    "print(\"\\nüìä Environment Information:\")\n",
    "print(f\"   Environment ID: {env.spec.id}\")\n",
    "print(f\"   Action dimensions: {env.action_space.shape}\")\n",
    "print(f\"   Observation dimensions: {env.observation_space.shape}\")\n",
    "print(f\"   Action bounds: [{env.action_space.low[0]:.2f}, {env.action_space.high[0]:.2f}]\")\n",
    "print(f\"   Wrapper stack: VecAdapter ‚Üí {'VecNormalize ‚Üí ' if USE_VECNORMALIZE else ''}VecMonitor\")\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0845d",
   "metadata": {},
   "source": [
    "## 4. Define PPO Model Architecture\n",
    "\n",
    "Configure the policy network architecture and hyperparameters for PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a773d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE OR LOAD PPO MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ü§ñ Setting up PPO model...\")\n",
    "\n",
    "# Create policy kwargs using our utility function\n",
    "policy_kwargs = create_policy_kwargs()\n",
    "\n",
    "print(\"üèóÔ∏è Policy Architecture:\")\n",
    "print(f\"   Activation: {policy_kwargs['activation_fn'].__name__}\")\n",
    "print(f\"   Network architecture: {policy_kwargs['net_arch']}\")\n",
    "print(f\"   Log std initialization: {policy_kwargs['log_std_init']}\")\n",
    "\n",
    "# Create or load model using our utility function\n",
    "model, env = create_or_load_model(\n",
    "    model_save_path=MODEL_SAVE_PATH,\n",
    "    env=env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    use_vecnormalize=USE_VECNORMALIZE,\n",
    "    force_new=FORCE_NEW,\n",
    "    continue_training=CONTINUE_TRAINING\n",
    ")\n",
    "\n",
    "# Update vecnormalize wrapper reference if modified\n",
    "if USE_VECNORMALIZE and vecnormalize_wrapper is None:\n",
    "    vecnormalize_wrapper = find_vecnormalize_wrapper(env)\n",
    "\n",
    "# Set the logger\n",
    "model.set_logger(logger)\n",
    "\n",
    "print(\"\\nüéØ PPO Hyperparameters:\")\n",
    "print(f\"   Learning rate: {model.learning_rate}\")\n",
    "print(f\"   Clip range: {model.clip_range}\")\n",
    "print(f\"   Target KL: {model.target_kl}\")\n",
    "print(f\"   Steps per rollout: {model.n_steps}\")\n",
    "print(f\"   Batch size: {model.batch_size}\")\n",
    "print(f\"   Epochs per update: {model.n_epochs}\")\n",
    "print(f\"   Gamma (discount): {model.gamma}\")\n",
    "print(f\"   GAE lambda: {model.gae_lambda}\")\n",
    "print(f\"   Max gradient norm: {model.max_grad_norm}\")\n",
    "print(f\"   Entropy coefficient: {model.ent_coef}\")\n",
    "print(f\"   Value function coefficient: {model.vf_coef}\")\n",
    "\n",
    "print(\"‚úÖ Model setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a189729",
   "metadata": {},
   "source": [
    "## 5. Train the PPO Model\n",
    "\n",
    "Execute the training loop with progress monitoring and interrupt handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c99d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üöÄ Starting PPO training...\")\n",
    "print(f\"üéØ Target timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(\"üí° Tip: Press 'Interrupt' button to stop training and save the model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Record training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Start training with progress monitoring\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "    \n",
    "    # Training completed successfully\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(\"üéâ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è Total training time: {training_duration:.2f} seconds ({training_duration/3600:.2f} hours)\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    # Handle graceful interruption\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user!\")\n",
    "    print(f\"‚è±Ô∏è Training time before interruption: {training_duration:.2f} seconds\")\n",
    "    print(\"üíæ Saving model before exit...\")\n",
    "    \n",
    "    # Save the model immediately\n",
    "    save_model_and_stats(model, MODEL_SAVE_PATH, vecnormalize_wrapper)\n",
    "    print(\"‚úÖ Model saved successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    print(\"üíæ Attempting to save model...\")\n",
    "    save_model_and_stats(model, MODEL_SAVE_PATH, vecnormalize_wrapper)\n",
    "    raise\n",
    "\n",
    "print(\"‚úÖ Training phase complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b22d6",
   "metadata": {},
   "source": [
    "## 6. Save Model and Normalization Statistics\n",
    "\n",
    "Save the trained model and VecNormalize statistics for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE TRAINED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üíæ Saving trained model and statistics...\")\n",
    "\n",
    "# Save model and normalization statistics\n",
    "save_model_and_stats(model, MODEL_SAVE_PATH, vecnormalize_wrapper)\n",
    "\n",
    "# Verify saved files\n",
    "model_file = f\"{MODEL_SAVE_PATH}.zip\"\n",
    "vecnorm_file = f\"{MODEL_SAVE_PATH}_vecnormalize.pkl\"\n",
    "\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "if os.path.exists(model_file):\n",
    "    model_size = os.path.getsize(model_file) / (1024 * 1024)  # MB\n",
    "    print(f\"   ‚úÖ Model: {model_file} ({model_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Model file not found: {model_file}\")\n",
    "\n",
    "if USE_VECNORMALIZE and os.path.exists(vecnorm_file):\n",
    "    vecnorm_size = os.path.getsize(vecnorm_file) / 1024  # KB\n",
    "    print(f\"   ‚úÖ VecNormalize: {vecnorm_file} ({vecnorm_size:.2f} KB)\")\n",
    "elif USE_VECNORMALIZE:\n",
    "    print(f\"   ‚ùå VecNormalize file not found: {vecnorm_file}\")\n",
    "\n",
    "print(\"‚úÖ Model saving complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51726c94",
   "metadata": {},
   "source": [
    "## 7. Load Trained Model for Evaluation\n",
    "\n",
    "Load the saved model and configure the evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP EVALUATION ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Setting up evaluation environment...\")\n",
    "\n",
    "# Create evaluation environment (typically single environment for cleaner results)\n",
    "eval_env = setup_environment(\n",
    "    env_name=ENV_NAME,\n",
    "    num_envs=1,  # Single environment for evaluation\n",
    "    seed=SEED + 1000,  # Different seed for evaluation\n",
    "    render_mode=None  # No rendering during evaluation\n",
    ")\n",
    "\n",
    "# Load model and normalization for evaluation\n",
    "eval_model, eval_env = load_model_and_normalization(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    env=eval_env,\n",
    "    auto_detect_vecnorm=True  # Automatically detect VecNormalize file\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation environment ready!\")\n",
    "print(f\"   Model loaded from: {MODEL_SAVE_PATH}.zip\")\n",
    "print(f\"   Evaluation environment: {ENV_NAME}\")\n",
    "print(f\"   Number of evaluation episodes: {N_EVAL_EPISODES}\")\n",
    "print(f\"   Deterministic policy: {DETERMINISTIC_EVAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e8786",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Run comprehensive evaluation and analyze performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN DETAILED EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä Running detailed evaluation...\")\n",
    "\n",
    "# Perform comprehensive evaluation\n",
    "eval_results = detailed_evaluation(\n",
    "    model=eval_model,\n",
    "    env=eval_env,\n",
    "    n_eval_episodes=N_EVAL_EPISODES,\n",
    "    deterministic=DETERMINISTIC_EVAL,\n",
    "    verbose=True  # Show episode-by-episode results\n",
    ")\n",
    "\n",
    "# Print comprehensive results\n",
    "print_evaluation_results(\n",
    "    results=eval_results,\n",
    "    env_name=ENV_NAME,\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    n_eval_episodes=N_EVAL_EPISODES\n",
    ")\n",
    "\n",
    "# Save evaluation results to file\n",
    "results_file = save_evaluation_results(\n",
    "    results=eval_results,\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    env_name=ENV_NAME,\n",
    "    n_eval_episodes=N_EVAL_EPISODES\n",
    ")\n",
    "\n",
    "# Store results for visualization\n",
    "episode_rewards = eval_results['episode_rewards']\n",
    "episode_lengths = eval_results['episode_lengths']\n",
    "mean_reward = eval_results['mean_reward']\n",
    "std_reward = eval_results['std_reward']\n",
    "\n",
    "print(\"‚úÖ Evaluation complete!\")\n",
    "print(f\"üìÑ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ece22",
   "metadata": {},
   "source": [
    "## 9. Visualize Training and Evaluation Results\n",
    "\n",
    "Create comprehensive visualizations of model performance and training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìà Creating visualizations...\")\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ---- Subplot 1: Episode Rewards Distribution ----\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(episode_rewards, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(mean_reward, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_reward:.2f}')\n",
    "plt.axvline(np.median(episode_rewards), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(episode_rewards):.2f}')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Episode Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Subplot 2: Episode Rewards Over Time ----\n",
    "plt.subplot(2, 3, 2)\n",
    "episodes = range(1, len(episode_rewards) + 1)\n",
    "plt.plot(episodes, episode_rewards, 'o-', alpha=0.7, color='blue', markersize=4)\n",
    "plt.axhline(mean_reward, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_reward:.2f}')\n",
    "plt.fill_between(episodes, mean_reward - std_reward, mean_reward + std_reward, alpha=0.2, color='red', label=f'¬±1 STD')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Episode Rewards Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Subplot 3: Episode Lengths Distribution ----\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(episode_lengths, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "plt.axvline(np.mean(episode_lengths), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(episode_lengths):.1f}')\n",
    "plt.axvline(np.median(episode_lengths), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(episode_lengths):.1f}')\n",
    "plt.xlabel('Episode Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Episode Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Subplot 4: Performance Summary Box Plot ----\n",
    "plt.subplot(2, 3, 4)\n",
    "box_data = [episode_rewards]\n",
    "bp = plt.boxplot(box_data, labels=['Rewards'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "bp['medians'][0].set_color('red')\n",
    "bp['medians'][0].set_linewidth(2)\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Performance Summary')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"\"\"Statistics:\n",
    "Mean: {mean_reward:.2f} ¬± {std_reward:.2f}\n",
    "Min: {np.min(episode_rewards):.2f}\n",
    "Max: {np.max(episode_rewards):.2f}\n",
    "Median: {np.median(episode_rewards):.2f}\n",
    "Success Rate: {np.sum(np.array(episode_rewards) > 0) / len(episode_rewards) * 100:.1f}%\"\"\"\n",
    "plt.text(1.1, np.median(episode_rewards), stats_text, fontsize=10, verticalalignment='center')\n",
    "\n",
    "# ---- Subplot 5: Reward vs Episode Length Scatter ----\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(episode_lengths, episode_rewards, alpha=0.6, color='purple', s=50)\n",
    "plt.xlabel('Episode Length')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Reward vs Episode Length')\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(episode_lengths, episode_rewards)[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# ---- Subplot 6: Performance Metrics Summary ----\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics_names = ['Mean\\nReward', 'Std\\nReward', 'Min\\nReward', 'Max\\nReward', 'Mean\\nLength']\n",
    "metrics_values = [mean_reward, std_reward, np.min(episode_rewards), np.max(episode_rewards), np.mean(episode_lengths)]\n",
    "colors = ['blue', 'orange', 'red', 'green', 'purple']\n",
    "\n",
    "bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7)\n",
    "plt.title('Performance Metrics Summary')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'PPO Training Results - {ENV_NAME} Environment', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ FINAL TRAINING AND EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"Training Steps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"Parallel Environments: {NUM_ENVS}\")\n",
    "print(f\"VecNormalize Used: {USE_VECNORMALIZE}\")\n",
    "print(f\"Model Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"\\nüìä Evaluation Results ({N_EVAL_EPISODES} episodes):\")\n",
    "print(f\"   Mean Reward: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "print(f\"   Best Episode: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"   Worst Episode: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"   Success Rate (>0): {np.sum(np.array(episode_rewards) > 0) / len(episode_rewards) * 100:.1f}%\")\n",
    "print(f\"   Mean Episode Length: {np.mean(episode_lengths):.1f}\")\n",
    "print(f\"   Evaluation Time: {eval_results['eval_time']:.2f} seconds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clean up environments\n",
    "env.close()\n",
    "eval_env.close()\n",
    "print(\"üßπ Environments closed. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
