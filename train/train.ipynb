{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6bb1e2",
   "metadata": {},
   "source": [
    "Train a quadrupedal controller using PPO with EnvPool.\n",
    "\n",
    "This script always uses EnvPool's gym interface, wraps the EnvPool\n",
    "object using a VecAdapter (inspired by EnvPool's SB3 example) to be compatible with SB3,\n",
    "and converts the action and observation spaces to float32 to satisfy SB3's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e5f5a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from packaging import version\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Import the correct Box class based on the gym version\n",
    "import gym\n",
    "import envpool\n",
    "from envpool.python.protocol import EnvPool  # For type annotations\n",
    "\n",
    "# Import Gymnasium spaces explicitly\n",
    "import gymnasium\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecEnvWrapper, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.preprocessing import check_for_nested_spaces, is_image_space, is_image_space_channels_first\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "\n",
    "# Force PyTorch to use one thread (for speed)\n",
    "th.set_num_threads(1)\n",
    "is_legacy_gym = version.parse(gym.__version__) < version.parse(\"0.26.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd311ba",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b47ff703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VecAdapter(VecEnvWrapper):\n",
    "    \"\"\"\n",
    "    Convert an EnvPool object to a Stable-Baselines3-compatible VecEnv.\n",
    "    This adapter sets the number of environments from the EnvPool spec and\n",
    "    implements step_wait to handle terminal resets, attaching terminal observations.\n",
    "    Also converts spaces to be SB3-compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, venv: EnvPool):\n",
    "        # Set the number of environments from EnvPool's config.\n",
    "        venv.num_envs = venv.spec.config.num_envs\n",
    "        super().__init__(venv)\n",
    "        \n",
    "        # Convert the action space to Gymnasium's Box with float32 (SB3 requires this)\n",
    "        self.action_space = Box(\n",
    "            low=venv.action_space.low.astype(np.float32),\n",
    "            high=venv.action_space.high.astype(np.float32),\n",
    "            shape=venv.action_space.shape,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        \n",
    "        # Convert the observation space to float32 as well\n",
    "        # First, check if it's a Box space\n",
    "        if isinstance(venv.observation_space, (gym.spaces.Box, gymnasium.spaces.Box)):\n",
    "            self.observation_space = Box(\n",
    "                low=venv.observation_space.low.astype(np.float32),\n",
    "                high=venv.observation_space.high.astype(np.float32),\n",
    "                shape=venv.observation_space.shape,\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            # If not a Box space, keep the original (but this might cause issues)\n",
    "            self.observation_space = venv.observation_space\n",
    "    \n",
    "    def step_async(self, actions: np.ndarray) -> None:\n",
    "        self.actions = actions\n",
    "    \n",
    "    def reset(self):\n",
    "        if is_legacy_gym:\n",
    "            obs = self.venv.reset()\n",
    "        else:\n",
    "            obs = self.venv.reset()[0]\n",
    "        # Convert observations to numpy array (if not already) and ensure float32\n",
    "        obs = np.asarray(obs, dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def seed(self, seed: int = None) -> None:\n",
    "        # Seeding is set at EnvPool creation.\n",
    "        pass\n",
    "    \n",
    "    def step_wait(self):\n",
    "        if is_legacy_gym:\n",
    "            obs, rewards, dones, info_dict = self.venv.step(self.actions)\n",
    "        else:\n",
    "            obs, rewards, terms, truncs, info_dict = self.venv.step(self.actions)\n",
    "            dones = terms + truncs\n",
    "        \n",
    "        # Ensure observations are float32\n",
    "        obs = np.asarray(obs, dtype=np.float32)\n",
    "        \n",
    "        infos = []\n",
    "        for i in range(self.num_envs):\n",
    "            info_i = {key: info_dict[key][i] for key in info_dict.keys() if isinstance(info_dict[key], np.ndarray)}\n",
    "            if dones[i]:\n",
    "                info_i[\"terminal_observation\"] = obs[i]\n",
    "                if is_legacy_gym:\n",
    "                    reset_obs = self.venv.reset(np.array([i]))\n",
    "                else:\n",
    "                    reset_obs = self.venv.reset(np.array([i]))[0]\n",
    "                obs[i] = np.asarray(reset_obs, dtype=np.float32)\n",
    "            infos.append(info_i)\n",
    "        return obs, rewards, dones, infos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba981286",
   "metadata": {},
   "source": [
    "# Add logger and parse args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b39adcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /app/envpool/runs_csv/20250531_115519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment: quadruped_ppo_experiment\n",
      "INFO:root:Using EnvPool for environment Humanoid-v4 with 1 envs. Seed: 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Define parse_args(), but pass an empty list when calling from a notebook\n",
    "def parse_args(arg_list=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train a quadrupedal controller using EnvPool and PPO.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--env-name\",\n",
    "        type=str,\n",
    "        default=\"Humanoid-v4\",\n",
    "        help=\"EnvPool environment ID\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-envs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of parallel environments\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=0, help=\"Random seed\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--total-timesteps\",\n",
    "        type=int,\n",
    "        default=100_000,\n",
    "        help=\"Total training timesteps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tb-log-dir\",\n",
    "        type=str,\n",
    "        default=\"./logs\",\n",
    "        help=\"TensorBoard log directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-save-path\",\n",
    "        type=str,\n",
    "        default=\"./quadruped_ppo_model\",\n",
    "        help=\"Model save path\",\n",
    "    )\n",
    "\n",
    "    # If arg_list is None, let argparse pick them up from sys.argv.\n",
    "    # In a Jupyter notebook, you'll want to override that by passing [] or a custom list.\n",
    "    if arg_list is None:\n",
    "        return parser.parse_args()\n",
    "    else:\n",
    "        return parser.parse_args(arg_list)\n",
    "\n",
    "\n",
    "# 3. Call it in a Jupyter cell by explicitly passing an empty list (so it doesn’t try to parse Jupyter’s own flags).\n",
    "args = parse_args(arg_list=[])\n",
    "\n",
    "# 4. Now you can safely use everything:\n",
    "run_dir = os.path.join(\"/app/envpool/runs_csv\", datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Build a logger (stdout, log, tensorboard, csv)\n",
    "logger = configure(\n",
    "    run_dir, format_strings=(\"stdout\", \"log\", \"tensorboard\", \"csv\")\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(\"Experiment: quadruped_ppo_experiment\")\n",
    "logging.info(\n",
    "    f\"Using EnvPool for environment {args.env_name} with {args.num_envs} envs. Seed: {args.seed}\"\n",
    ")\n",
    "\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f75e5d",
   "metadata": {},
   "source": [
    "Create Env and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26855940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] [FSM_State_Passive.cpp:29] (void FSM_State_Passive<T>::onEnter() [with T = float]) - .\n",
      "Using cpu device\n",
      "[DEBUG] [FSM_State_StandUp.cpp:32] (void FSM_State_StandUp<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_BalanceStand.cpp:38] (void FSM_State_BalanceStand<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_Locomotion.cpp:58] (void FSM_State_Locomotion<T>::onEnter() [with T = float]) - .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create EnvPool environment using the gym interface.\n",
    "env = envpool.make(args.env_name, env_type=\"gym\", num_envs=args.num_envs, seed=args.seed)\n",
    "\n",
    "# Set environment ID without modifying action_space directly\n",
    "env.spec.id = args.env_name\n",
    "\n",
    "\n",
    "# Use the adapter which will handle the action_space and observation_space conversion\n",
    "env = VecAdapter(env)\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_reward=10.0)\n",
    "env = VecMonitor(env)  # Monitor for tracking episode stats\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "\"MlpPolicy\",\n",
    "env,\n",
    "learning_rate=5e-4,\n",
    "n_steps=1024,               # 128 × 8 = 1024     samples / iteration\n",
    "batch_size=256,\n",
    "gamma=0.95,\n",
    "gae_lambda=0.90,\n",
    "clip_range=0.2,\n",
    "ent_coef=0.01,\n",
    "vf_coef=0.25,\n",
    "max_grad_norm=0.5,\n",
    "policy_kwargs=dict(net_arch=[dict(pi=[16,16], vf=[16,16])]),\n",
    "verbose=1,\n",
    "tensorboard_log=\"runs/ppo_debug\",\n",
    ")\n",
    "model.set_logger(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acb99371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the environment...\n",
      "Unsafe locomotion: roll is 70.731 degrees (max 70.000)\n",
      "[DEBUG] [FSM_State_Passive.cpp:29] (void FSM_State_Passive<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_StandUp.cpp:32] (void FSM_State_StandUp<T>::onEnter() [with T = float]) - .\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149091, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149091, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "[DEBUG] [FSM_State_BalanceStand.cpp:38] (void FSM_State_BalanceStand<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_Locomotion.cpp:58] (void FSM_State_Locomotion<T>::onEnter() [with T = float]) - .\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.146633, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.146633, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "Unsafe locomotion: leg 3 is above hip (0.504 m)\n",
      "[DEBUG] [FSM_State_Passive.cpp:29] (void FSM_State_Passive<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_StandUp.cpp:32] (void FSM_State_StandUp<T>::onEnter() [with T = float]) - .\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.146278, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.146278, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "[DEBUG] [FSM_State_BalanceStand.cpp:38] (void FSM_State_BalanceStand<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_Locomotion.cpp:58] (void FSM_State_Locomotion<T>::onEnter() [with T = float]) - .\n",
      "Unsafe locomotion: roll is 70.208 degrees (max 70.000)\n",
      "[DEBUG] [FSM_State_Passive.cpp:29] (void FSM_State_Passive<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_StandUp.cpp:32] (void FSM_State_StandUp<T>::onEnter() [with T = float]) - .\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149909, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149909, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "[DEBUG] [FSM_State_BalanceStand.cpp:38] (void FSM_State_BalanceStand<T>::onEnter() [with T = float]) - .\n",
      "[DEBUG] [FSM_State_Locomotion.cpp:58] (void FSM_State_Locomotion<T>::onEnter() [with T = float]) - .\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.148425, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.148425, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149932, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.149932, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.459907, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "[IsHealthy] Unhealthy state detected: z position = 0.459907, healthy_z_min = 0.15, healthy_z_max = 0.45\n",
      "Resetting the environment...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model \u001b[39;00m\n\u001b[1;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/vec_normalize.py:181\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "Cell \u001b[0;32mIn[46], line 54\u001b[0m, in \u001b[0;36mVecAdapter.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     obs, rewards, dones, info_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     obs, rewards, terms, truncs, info_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     dones \u001b[38;5;241m=\u001b[39m terms \u001b[38;5;241m+\u001b[39m truncs\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Ensure observations are float32\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/envpool/python/envpool.py:142\u001b[0m, in \u001b[0;36mEnvPoolMixin.step\u001b[0;34m(self, action, env_id)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform one step with multiple environments in EnvPool.\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(action, env_id)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/envpool/python/envpool.py:128\u001b[0m, in \u001b[0;36mEnvPoolMixin.recv\u001b[0;34m(self, reset, return_info)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecv\u001b[39m(\n\u001b[1;32m    123\u001b[0m   \u001b[38;5;28mself\u001b[39m: EnvPool,\n\u001b[1;32m    124\u001b[0m   reset: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m   return_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TimeStep, Tuple]:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Recv a batch state from EnvPool.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m   state_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to(state_list, reset, return_info)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "logging.info(\"Starting training...\")\n",
    "model.learn(total_timesteps=args.total_timesteps)\n",
    "logging.info(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(args.model_save_path)\n",
    "logging.info(f\"Model saved at: {args.model_save_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87610c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the EnvPool environment.\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "print(f\"EnvPool Evaluation - {args.env_name}\")\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4474bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
